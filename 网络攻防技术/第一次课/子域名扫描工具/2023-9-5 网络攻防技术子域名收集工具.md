# 相关知识准备

## meta 标签的属性的作用

```html
<meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
```

	我们以“Content-Security-Policy”参数为例，设置参数值为：“upgrade-insecure-requests”，在https页面中，如果调用了http资源，那么浏览器就会抛出一些错误。为了改变成这一状况，chrome(谷歌浏览器)会在http请求中加入 ‘Upgrade-Insecure-Requests: 1’ ，服务器收到请求后会返回 “Content-Security-Policy: upgrade-insecure-requests” 头，告诉浏览器，**可以把所属本站的所有 http 连接升级为 https 连接。**

## 反爬机制突破策略
### 1. User-Agent字段
### 1.1. User-Agent 字段&User-Agent 的作用

User-Agent 字段是 `HTTP` 协议中的一个请求头部，通常用于标识客户端发送请求的应用程序、操作系统、设备类型、版本等信息。服务器可以根据 `User-Agent` 字段来判断请求是否来自合法的客户端，做出相应的响应，比如返回不同的网页内容、进行流量控制等。

下面是一些 `User-Agent` 字段的例子，重点对比其差异。

Chrome 浏览器的 User-Agent 字段：`Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36`
Firefox 浏览器的 User-Agent 字段：`Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:96.0) Gecko/20100101 Firefox/96.0`
Safari 浏览器的 User-Agent 字段：`Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.1 Safari/605.1.15`
微信小程序的 User-Agent 字段：`Mozilla/5.0 (iPhone; CPU iPhone OS 12_0 like Mac OS X) AppleWebKit/604.1.34 (KHTML, like Gecko) Mobile/16A366 MicroMessenger/7.0.10(0x17000a22) NetType/WIFI Language/zh_CN`
Python 爬虫的 User-Agent 字段`：Python-urllib/3.8`
### 1.2. 常见 User-Agent 的特征

常见 User-Agent 的特征包括以下几点：

- 操作系统信息：User-Agent 字段通常会包含操作系统的信息，比如 Windows、MacOS、Linux 等，不同的操作系统使用的浏览器可能不同，因此对于一些网站来说，操作系统的信息也可以作为反爬手段之一。

- 浏览器信息：User-Agent 字段还会包含浏览器的信息，比如 Chrome、Firefox、Safari 等。不同的浏览器在渲染网页时可能会有一些差别，因此一些网站会根据浏览器信息来判断是否是爬虫。

- 设备信息：User-Agent 字段有时也会包含设备信息，比如 iPhone、iPad 等。不同设备使用的浏览器也可能不同，因此设备信息也可以作为反爬手段之一。

- 自定义 User-Agent：有些爬虫工具允许用户自定义 User-Agent 字段，因此一些网站也会根据这些自定义 User-Agent 来判断是否是爬虫。

### 1.3. User-Agent 字段的伪装方式
在 Python 中伪装 User-Agent 字段，可以使用第三方库 `fake-useragent`，它可以随机生成不同浏览器的 User-Agent。

使用 pip 命令安装 `fake-useragent` 库：
`pip install fake-useragent

用法
```python
from fake_useragent import UserAgent

ua = UserAgent()
print(ua.random)
```

### 2.  Referer 字段
### 2.1. Referer 的作用
Referer 是 HTTP 请求头中的一个字段，用来表示当前请求是从哪个页面跳转过来的。
通常在请求图片、视频、音频等静态资源时，浏览器会自动添加 Referer 字段，用来告诉服务器当前资源是从哪个页面发起请求的，服务器就可以根据这个信息做一些统计、记录或者安全控制等操作。

Referer 字段的作用主要有以下几点：

- 防盗链：有些网站可能会设置一些图片或者视频等静态资源，只有在自己的网站上才能正常显示，如果从其他网站上直接访问，就会被拒绝访问。此时可以通过 Referer 字段来判断是否是从自己网站上来的请求，如果不是就拒绝访问。
- 流量统计：通过 Referer 字段可以统计来访者是从哪个页面过来的，从而了解不同页面的流量情况，对网站进行优化。
- 安全控制：有些网站可能会针对特定的 Referer 来源做一些安全控制，如防止 CSRF 攻击等。
### 2.2. Referer 的特征
Referer 是 HTTP 请求头的一部分，它记录了当前请求的来源页面，即发送请求的页面 URL。具体来说，当用户从一个页面 A 点击链接跳转到另一个页面 B 时，页面 B 的请求头中的 Referer 字段会记录页面 A 的 URL。

Referer 的特征主要包括：

- Referer 字段中记录的 URL 是前一次请求的 URL。因此，如果请求头中没有 Referer 字段，或者 Referer 字段中的 URL 与当前请求的 URL 不符，则可能被认为是异常请求，从而被服务器屏蔽或拒绝。

- Referer 字段可以用于防盗链，即通过判断 Referer 字段来判断是否是从合法的网站访问资源。比如，某个网站的图片资源只允许从该网站访问，而不允许其他网站直接链接该资源。这时，服务器可以通过判断 Referer 字段来防止其他网站直接链接该资源。

- Referer 字段可能会泄漏用户的隐私信息，因为它可以记录用户的浏览历史。为了保护用户的隐私，一些浏览器可能会限制 Referer 字段的发送，或者发送空的 Referer 字段。

### 2.3. Referer 字段的伪装方式
要伪装 Referer 字段，可以通过构造请求头部实现。下面是用 Python 实现的示例代码：

```python
import requests

url = 'https://pachong.vip'
referer = 'https://www.referer.com'

headers = {
    'Referer': referer,
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'
}

response = requests.get(url, headers=headers)
```

在请求头中添加 Referer 字段，值为想要伪装成的来源页面的 URL。以上代码中，将 Referer 设置为 https://www.referer.com。

如果设置的 Referer 不符合实际情况，有可能会被网站识别为爬虫，导致请求被拒绝。因此，在进行爬虫时，应该根据具体情况，合理设置 Referer 字段。

### 3. Cookie 字段
### 3.1. Cookie 的作用
Cookie 是 Web 服务器发送给 Web 浏览器的小型文本文件，用于存储用户在浏览器中的状态信息。它可以使网站记住用户的登录状态、购物车内容、浏览历史等。

当用户访问一个网站时，Web 服务器会在 HTTP 响应头中添加 Set-Cookie 头信息，浏览器接收到响应后，会将 Cookie 保存在本地。当用户再次访问该网站时，浏览器会将保存的 Cookie 信息发送给 Web 服务器，服务器根据 Cookie 中的信息来进行相应的处理，比如保持用户登录状态等。

Cookie 的作用可以简单总结为三点：

- 记录用户的状态信息，如登录状态、购物车内容、浏览历史等；
- 使用户在访问同一个网站时能够保持同一个状态，比如不需要重复登录；
- 用于跟踪用户行为，帮助网站分析用户兴趣、推荐相关内容。
### 3.2. Cookie 的特征

- 存储在客户端：Cookie 是存储在客户端浏览器中的文本文件，可以通过浏览器进行查看和修改。

- 用于状态管理：Cookie 主要用于状态管理，例如网站登录状态的保存，购物车中的商品信息的保存等。

- 可以设置过期时间：可以在创建 Cookie 时设置过期时间，使得浏览器可以在过期时间之前将其删除。

- 每个域名单独存储：每个域名在客户端上都有一个单独的 Cookie 存储空间，不同域名之间的 Cookie 互不干扰。

- 大小限制：Cookie 存储的数据量有大小限制，通常为 4KB 左右。

- 不安全性：Cookie 存储的数据都是明文的，可能会被恶意篡改或者窃取。因此，敏感信息不应该存储在 Cookie 中。

- 可以禁用：浏览器允许用户手动禁用 Cookie，因此不能完全依赖 Cookie 进行状态管理。

### 3.3. Cookie 字段的伪装方式

手动设置 Cookie：可以在请求头中手动设置 Cookie 字段，模拟浏览器发送的请求。具体实现方式为，在构建 HTTP 请求时，添加请求头中的 Cookie 字段并填入具体的 Cookie 值。
```python
import requests

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',
    'Cookie': 'name=value'
}

response = requests.get(url, headers=headers)
```

使用第三方库：可以使用第三方库进行 Cookie 伪装。例如，使用 `fake_useragent` 库伪装 User-Agent 和 `http.cookiejar` 库伪装 Cookie。

```python
import requests
from fake_useragent import UserAgent
import http.cookiejar as cookielib

# 构建cookiejar对象
cookie_jar = cookielib.CookieJar()

# 构建opener
opener = requests.build_opener(requests.HTTPCookieProcessor(cookie_jar))

# 构建headers
headers = {'User-Agent': UserAgent().random}

# 发送请求
response = opener.open(url, headers=headers)
```

使用 Selenium 模拟浏览器：使用 Selenium 可以直接在浏览器中登录网站获取 Cookie，然后使用获取到的 Cookie 来发送请求。

```python
from selenium import webdriver

# 构建浏览器对象
browser = webdriver.Chrome()

# 打开网站并登录
browser.get(url)

# 获取Cookie
cookies = browser.get_cookies()

# 构建cookie字符串
cookie_str = ''
for cookie in cookies:
    cookie_str += cookie['name'] + '=' + cookie['value'] + ';'

# 构建请求头
headers = {'Cookie': cookie_str}

# 发送请求
response = requests.get(url, headers=headers)
```
# 实验步骤
## 分析过程

本次实验目的是收集子域名，首先对百度的搜索策略进行查询得到查询域名的策略为`site:[]`。
原本以为百度会像必应一样直接给出搜索结果跳转链接，但是发现百度对跳转链接进行了加密。

![[截屏2023-09-05 16.21.28.png]]

尝试了解其加密方式：(参考链接 https://blog.csdn.net/liuxl57805678/article/details/125310578)
```php
function _empty($param)
{
if (empty($param) && !is_numeric($param)) {
    return true;
} else {
    return false;
}
}
 
function encryption($domain) {
 
    if (empty($domain)) return $domain;
 
    //$ascii码表x,y位置
    $ascii = [
        [' ', '!', '"', '#', '$', '%', '&', "'", '(', ')', '*', '+', ',', '-', '.', '/'],
        ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?'],
        ['@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O'],
        ['P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\', ']', '^', '_'],
        ['`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o'],
        ['p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', ' ']];
 
    $data  = [
        [13, 10],
        [10, 1],
        [16, 15],
        [7, 6],
        [5, 13],
        [18, 9],
        [13, 11],
        [7, 12],
        [10, 5],
        [15, 9],
        [15, 9],
        [13, 5],
        [16, 11],
        [0, 1],
        [8, 2],
        [8, 5],
        [0, 0],
        [17, 15],
        [14, 8],
        [2, 1],
        [10, 5],
        [17, 14],
        [16, 5],
        [3, 8],
        [14, 9],
        [5, 8],
        [15, 1],
        [3, 15],
        [13, 10],
        [10, 12],
        [5, 7],
        [0, 2],
        [18, 14],
        [0, 15],
        [1, 6],
        [13, 5],
        [2, 1],
        [15, 14],
        [18, 8],
        [18, 9],
        [1, 10],
        [14, 14],
        [13, 2],
        [5, 3],
        [5, 8],
        [0, 4],
        [1, 5],
        [16, 1],
        [8, 1],
        [2, 5],
        [10, 7],
        [10, 15],
        [14, 14],
        [17, 3],
        [15, 0],
        [14, 5],
        [7, 7],
        [3, 4],
        [14, 8],
        [12, 0],
        [13, 12],
        [12, 3],
        [6, 5],
        [3, 1],
        [1, 14],
        [5, 4],
        [ 0, 12],
        [7, 0],
        [10, 7],
        [15, 12],
        [8, 2],
        [18, 15],
        [3, 12],
        [1, 12],
        [0, 15],
        [17, 4],
        [17, 2],
        [11, 1],
        [3, 12],
        [11, 5],
        [0, 13],
        [1, 1],
        [2, 12]];
    //所有的y值
 
    $y = [
        '0123456789abcdef',
        '1032547698badcfe',
        '23016745ab89efcd',
        '32107654ba98fedc',
        '45670123cdef89ab',
        '54761032dcfe98ba',
        '67452301efcdab89',
        '76543210fedcba98',
        '89abcdef01234567',
        '98badcfe10325476',
        'ab89efcd23016745',
        'ba98fedc32107654',
        'cdef89ab45670123',
        'dcfe98ba54761032',
        'efcdab8967452301',
        'fedcba9876543210'];
    //所有的x值
    $x = [
        '016745',
        '107654',
        '234567',
        '321076',
        '325476',
        '452301',
        '543210',
        '670123',
        '765432',
        '761032',
        '89abcd',
        '89efcd',
        '98fedc',
        'abcdef',
        'badcfe',
        'cdab89',
        'dcba98',
        'ef89ab',
        'fe98ba'];
 
    //解密过程
    $len = strlen($domain);
    $mi = '';
 
    for($i=0; $i < $len; $i++) {
        $ch  = substr($domain,$i,1);
        $f_real_x = '';
        $f_real_y = '';
        foreach ($ascii as $real_x=>$real_y_chs) {
            foreach ($real_y_chs as $real_y=>$real_y_ch) {
                if ($real_y_ch == $ch) {
                    $f_real_x = $real_x;
                    $f_real_y = $real_y;
                    break;
                }
            }
        }
    
        if (!_empty($f_real_x) && !_empty($f_real_y)) {
            $this_data_x = $x[$data[$i][0]];
            $encode_x = $this_data_x[$f_real_x];
            $mi .=  $encode_x;
            $this_data_y = $y[$data[$i][1]];
            $encode_y = $this_data_y[$f_real_y];
            $mi .= $encode_y;
        }
    }
 
    return 'http://www.baidu.com/link?url=a3f48d30fc293c5e471ef23de092fddc99'.$mi;
 
}
 
echo encryption('www.php.net');
```

## 功能实现
### 获取跳转后的链接
过于复杂，于是想到可以模仿正常的用户对其进行点击，获得跳转后的链接，再获取其真实url即可。所以在这里我们运用requests库中allow_redirects=true来获得跳转后的链接。

![[截屏2023-09-05 16.21.51.png]]

### 伪造headers字段

根据预备知识我们知道，为了突破反爬虫机制，我们可以伪造headers来欺骗浏览器，使得其以为是真实人类在访问。在伪造headers时，我们可以利用fake_useragent库来实现每次访问都是不同的User-Agent字段，更具备真实性。

![[截屏2023-09-05 16.25.55.png]]

### 获取真实cookie，模拟登陆

很多网页采用Cookie登陆，利用该特性可以帮助我们减少爬取受到的限制。这里我们采用selenium库，调用Chrome实体模拟正常搜寻得到需要的Cookie：

![[截屏2023-09-05 19.46.09.png]]

至此，我们可以有较大把握我们的子域名收集工具能够成功运行。至于收集的域名如何处理，详情见代码注释，大体与样例相同。

### 返回网页基础信息

标题、链接、描述同样通过BeautifulSoup包抓取相关标签获得。

![[截屏2023-09-06 21.30.49.png]]
## 实验结果

完成基本功能以及拓展功能：爬取子域名+标题+描述
突破反爬机制（虽然还会有遇到安全验证的情形，后续解决办法由自行尝试后上传至GitHub）

![[截屏2023-09-06 21.43.04.png]]

![[截屏2023-09-05 20.29.40.png]]

经过对搜索URL的分析，发现有几个关键字段发生了变化，但是查询相关资料后发现其为未知含义的加密数据。

![[截屏2023-09-06 21.46.33.png]]

![[截屏2023-09-06 21.27.08.png]]

在查询rsv_pq的过程中，发现rsv_enter字段是用于检测键盘敲击次数。这说明百度检验是否为机器人的方式实在多样，传统的爬虫可能根本无法通过其安全机制的检查。

TODO:
- 构建代理IP池，通过IP的更迭避免IP封锁
- 弄清rsv_pq以及rsv_t的含义以及产生机制，进而更清晰地构造链接
- 尝试selenium包进行真实浏览行为模拟